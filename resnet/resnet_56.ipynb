{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet-56.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPbuK2ajcbofr1VLFRriJ2y"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yshM2mkPWStu"
      },
      "source": [
        "Mount drive for checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eItLLevEWWLB"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!ls \"/content/gdrive/MyDrive/cnn-architectures/resnet\"\n",
        "root_path = '/gdrive/MyDrive/cnn-architectures/resnet'\n",
        "path = '/content/gdrive/MyDrive/cnn-architectures/resnet'\n",
        "os.chdir(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG5eAdkyFkQo"
      },
      "source": [
        "Standard imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt6feR1xED8G"
      },
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W71m4m10ZfoB"
      },
      "source": [
        "Initialize weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "no3lNSLdZhbR"
      },
      "source": [
        "#use kaiming initialization\n",
        "def _weights_init(m):\n",
        "    \"\"\"\n",
        "        Initialization of CNN weights\n",
        "    \"\"\"\n",
        "    classname = m.__class__.__name__\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4TwUphbZrOe"
      },
      "source": [
        "Identity layer (Lambda layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Suhrb-k6ZuW8"
      },
      "source": [
        "class IdentityConn(nn.Module):\n",
        "    \"\"\"\n",
        "      Identity mapping between ResNet blocks with different sized feature maps\n",
        "    \"\"\"\n",
        "    def __init__(self, lambd):\n",
        "        super(IdentityConn, self).__init__()\n",
        "        self.lambd = lambd  #since lambda is a python keyword\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1pB9niBZ6CC"
      },
      "source": [
        "Basic block"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvvna0I1Z74-"
      },
      "source": [
        "'''\n",
        "Consists of 2 convolutional blocks each of which is followed by a Batch-norm layer.\n",
        "Each basic block is \"short-circuited\" to create the identity mapping.\n",
        "'''\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1): \n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "          self.shortcut = IdentityConn(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1lsBIfla_Y0"
      },
      "source": [
        "ResNet class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atK20kd0bAm2"
      },
      "source": [
        "'''\n",
        "3 stacks of 2*n (n = number of basic blocks) layers\n",
        "each of the 2n layers have feature maps of size {16,32,64} \n",
        "a stride of 2 is used for subsampling while performing the convolution\n",
        "'''\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gTJZWatbj3d"
      },
      "source": [
        "ResNet 56 definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-y6uoLcbnZV"
      },
      "source": [
        "def resnet56():\n",
        "    return ResNet(BasicBlock, [9, 9, 9])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ur5OEwIDcWaM"
      },
      "source": [
        "Set hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tR0XtiycYFo"
      },
      "source": [
        "class MyResNetArgs:\n",
        "   def __init__(self, arch='resnet56' ,epochs=200, start_epoch=0, batch_size=128, lr=0.1, momentum=0.9, weight_decay=1e-4, print_freq=55,\n",
        "                 evaluate=0, pretrained=0, save_dir='save_temp', save_every=10):\n",
        "        self.save_every = save_every        #Saves checkpoints at every specified number of epochs\n",
        "        self.save_dir = save_dir            #The directory used to save the trained models\n",
        "        self.evaluate = evaluate            #evaluate model on the validation set\n",
        "        self.pretrained = pretrained        #evaluate the pretrained model on the validation set\n",
        "        self.print_freq = print_freq        #print frequency \n",
        "        self.weight_decay = weight_decay\n",
        "        self.momentum = momentum \n",
        "        self.lr = lr                        #Learning rate\n",
        "        self.batch_size = batch_size \n",
        "        self.start_epoch = start_epoch\n",
        "        self.epochs = epochs\n",
        "        self.arch = arch                    #ResNet model used"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6yLRHPqcvwx"
      },
      "source": [
        "Model summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo-cCZbOcxYS"
      },
      "source": [
        "args = MyResNetArgs('resnet56',pretrained=0)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "\n",
        "model = resnet56().to(device)\n",
        "\n",
        "summary(model, (3,32,32))\n",
        "best_prec1 = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSHzwDbPgHeC"
      },
      "source": [
        "Train function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI98GYVEgJK-"
      },
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "\n",
        "    batch_time = AvgCalc()\n",
        "    data_time = AvgCalc()\n",
        "    losses = AvgCalc()\n",
        "    top1 = AvgCalc()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "\n",
        "        # measure data loading time\n",
        "        data_time.update(time.time() - end)\n",
        "        target = target.cuda()\n",
        "        input_var = input.cuda()\n",
        "        target_var = target\n",
        "        if args.half:\n",
        "            input_var = input_var.half()\n",
        "\n",
        "        # compute output\n",
        "        output = model(input_var)\n",
        "        loss = criterion(output, target_var)\n",
        "\n",
        "        # compute gradient and perform one iteration of SGD\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        output = output.float()\n",
        "        loss = loss.float()\n",
        "\n",
        "        # measure accuracy and record loss (top-1%)\n",
        "        prec1 = accuracy(output.data, target)[0]\n",
        "        losses.update(loss.item(), input.size(0))\n",
        "        top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "        # measure elapsed time\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "        \n",
        "        if i % args.print_freq == 0:\n",
        "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
        "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
        "                      data_time=data_time, loss=losses, top1=top1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egDDC9pVgujm"
      },
      "source": [
        "Validation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUiwnfbDgv39"
      },
      "source": [
        "def validate(val_loader, model, criterion):\n",
        "    '''\n",
        "      print the top-k classification accuracy and error\n",
        "    '''\n",
        "    \n",
        "    batch_time = AvgCalc  losses = AvgCalc()\n",
        "    top1 = AvgCalc()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    end = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "            target = target.cuda()\n",
        "            input_var = input.cuda()\n",
        "            target_var = target.cuda()\n",
        "\n",
        "            if args.half:\n",
        "                input_var = input_var.half()\n",
        "\n",
        "            # compute output\n",
        "            output = model(input_var)\n",
        "            loss = criterion(output, target_var)\n",
        "            output = output.float()\n",
        "            loss = loss.float()\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            prec1 = accuracy(output.data, target)[0]\n",
        "            losses.update(loss.item(), input.size(0))\n",
        "            top1.update(prec1.item(), input.size(0))\n",
        "\n",
        "            # measure elapsed time\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "\n",
        "    print('Test\\t  Prec@1: {top1.avg:.3f} (Err: {error:.3f} )\\n'\n",
        "          .format(top1=top1,error=100-top1.avg))\n",
        "\n",
        "    return top1.avg\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66STQBAqhGPV"
      },
      "source": [
        "Save progress"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoLE9cHQhHTk"
      },
      "source": [
        "def save_checkpoint(state, filename='checkpoint.th'):\n",
        "    torch.save(state, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2ZUxs4WhQ4r"
      },
      "source": [
        "Average accuracy of mini-batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTiWc0xahUGZ"
      },
      "source": [
        "class AvgCalc(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3BpZeTwhcVY"
      },
      "source": [
        "Top-k precision at specified k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uffm9c7he7r"
      },
      "source": [
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8VyEfd2OAmR"
      },
      "source": [
        "Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQKDEFt0ODKP"
      },
      "source": [
        "#normalize the images\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "\n",
        "#Training data loader\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(32, 4),\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ]), download=True),\n",
        "        batch_size=args.batch_size, shuffle=True,\n",
        "        num_workers=4)\n",
        "\n",
        "#Validation data loader\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "        datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            normalize,\n",
        "        ])),\n",
        "        batch_size=128, shuffle=False,\n",
        "        num_workers=4)\n",
        "\n",
        "#Specify classes\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaeX6PzDhpzv"
      },
      "source": [
        "Experiment..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doLT4Bahhsk5"
      },
      "source": [
        "def main():\n",
        "    global args, best_prec1\n",
        "    \n",
        "    # Check whether save_dir exists or not\n",
        "    if not os.path.exists(args.save_dir):\n",
        "        os.makedirs(args.save_dir)\n",
        "    model = resnet56()\n",
        "    model.cuda()\n",
        "\n",
        "    # define loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss().cuda()\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n",
        "                                momentum=args.momentum,\n",
        "                                weight_decay=args.weight_decay)\n",
        "\n",
        "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer,\n",
        "                                                        milestones=[100, 150], last_epoch=args.start_epoch - 1)\n",
        "    \n",
        "    if args.evaluate:\n",
        "        print('evaluate')\n",
        "        model.load_state_dict(torch.load(os.path.join(args.save_dir, 'model.th')))\n",
        "        best_prec1 = validate(val_loader, model, criterion)\n",
        "        return best_prec1\n",
        "    \n",
        "    for epoch in range(args.start_epoch, args.epochs):\n",
        "\n",
        "        print('Learning rate {:.5e}'.format(optimizer.param_groups[0]['lr']))\n",
        "        train(train_loader, model, criterion, optimizer, epoch)\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        #evaluate on validation set\n",
        "        prec1 = validate(val_loader, model, criterion)\n",
        "\n",
        "        #best precision and save checkpoint\n",
        "        is_best = prec1 > best_prec1\n",
        "        best_prec1 = max(prec1, best_prec1)\n",
        "\n",
        "        if epoch > 0 and epoch % args.save_every == 0:\n",
        "            save_checkpoint(model.state_dict(), filename=os.path.join(args.save_dir, 'checkpoint.th'))\n",
        "        if is_best:\n",
        "            save_checkpoint(model.state_dict(), filename=os.path.join(args.save_dir, 'model.th'))\n",
        "\n",
        "    return best_prec1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_p5_0HJif6V"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "   best_prec1 = main()\n",
        "   print('The lowest error from {} model after {} epochs is {error:.3f}'.format(args.arch,args.epochs,error=100-best_prec1)) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}